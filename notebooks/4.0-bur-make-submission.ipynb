{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vesuvius Challenge - Ink Detection: Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from glob import glob\n",
    "from os import sep\n",
    "from os.path import join, abspath\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "MODELS_DIR = join(os.pardir, 'models')\n",
    "KAGGLE_INPUT_DIR = join(abspath(sep), 'kaggle', 'input')\n",
    "TEST_FRAGMENTS_PATH = join(KAGGLE_INPUT_DIR, 'vesuvius-challenge-ink-detection', 'test')\n",
    "TEST_FRAGMENTS_PATH = join(os.pardir, 'data', 'raw', 'test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vesuvius WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(os.pardir, 'data', 'raw', 'wandb', 'wandb_export.csv'))\n",
    "df.set_index('ID', inplace=True)\n",
    "df['start_slice'] = df['start_slice'].astype(int)\n",
    "df['num_slices'] = df['num_slices'].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_output(tiles, bboxes, fragment_id, fragment_shape, tile_size):\n",
    "    reconstructed_output = torch.zeros(fragment_shape).to(device=tiles.device)\n",
    "    count_map = torch.zeros(fragment_shape).to(device=tiles.device)\n",
    "\n",
    "    for i in range(tiles.shape[0]):\n",
    "        x0, y0, x1, y1 = bboxes[i]\n",
    "        reconstructed_output[y0:y1, x0:x1] += tiles[i, :, :]\n",
    "        count_map[y0:y1, x0:x1] += 1\n",
    "\n",
    "    reconstructed_output /= count_map\n",
    "    reconstructed_output = torch.nan_to_num(reconstructed_output, nan=0)\n",
    "\n",
    "    mask_path = os.path.join(TEST_FRAGMENTS_PATH, fragment_id, 'mask.png')\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    padding = get_padding(mask.shape, tile_size)\n",
    "\n",
    "    shape = reconstructed_output.shape\n",
    "    x0, y0, x1, y1 = padding[1][0], padding[0][0], shape[1] - padding[1][1], shape[0] - padding[0][1]\n",
    "    reconstructed_output = reconstructed_output[y0:y1, x0:x1]\n",
    "\n",
    "    return reconstructed_output\n",
    "\n",
    "\n",
    "def get_fragment_shape(fragment_dir, fragment_id, tile_size):\n",
    "    mask_path = os.path.join(fragment_dir, fragment_id, 'mask.png')\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    padding = get_padding(mask.shape, tile_size)\n",
    "    mask_pad = np.pad(mask, padding)\n",
    "\n",
    "    return mask_pad.shape\n",
    "\n",
    "\n",
    "def get_padding(mask_shape, tile_size, overlap=0.5):\n",
    "    pad_left = int(overlap * tile_size)\n",
    "    pad_up = int(overlap * tile_size)\n",
    "    pad_right = int(overlap * tile_size + tile_size - mask_shape[1] % tile_size)\n",
    "    pad_down = int(overlap * tile_size + tile_size - mask_shape[0] % tile_size)\n",
    "    padding = [(pad_up, pad_down), (pad_left, pad_right)]\n",
    "\n",
    "    return padding\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "\n",
    "    return torch.device(device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vesuvius Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "class LightningVesuvius(pl.LightningModule):\n",
    "    def __init__(self, model_params):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = smp.Unet(**model_params)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model(inputs)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vesuvius Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class DatasetVesuvius(Dataset):\n",
    "    def __init__(self, fragments, tile_size, num_slices, slices_list, start_slice, reverse_slices, selection_thr, augmentation, device, overlap):\n",
    "        self.fragments = fragments\n",
    "        self.tile_size = tile_size\n",
    "        self.num_slices = num_slices\n",
    "        self.slices_list = slices_list\n",
    "        self.start_slice = start_slice\n",
    "        self.reverse_slices = reverse_slices\n",
    "        self.selection_thr = selection_thr\n",
    "        self.augmentation = augmentation\n",
    "        self.device = device\n",
    "\n",
    "        self.overlap = overlap\n",
    "        self.set_path = TEST_FRAGMENTS_PATH\n",
    "        self.slices = self.make_slices()\n",
    "        self.data, self.items = self.make_data()\n",
    "\n",
    "        self.transforms = T.RandomApply(\n",
    "            nn.ModuleList([\n",
    "                T.RandomRotation(180),\n",
    "                T.RandomPerspective(),\n",
    "                T.ElasticTransform(alpha=500.0, sigma=10.0),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomVerticalFlip()\n",
    "            ]), p=0.5\n",
    "        )\n",
    "\n",
    "    def make_slices(self):\n",
    "        total_slices = 65\n",
    "        slices = [i for i in range(total_slices)]\n",
    "\n",
    "        if self.slices_list:\n",
    "            slices = self.slices_list\n",
    "        else:\n",
    "            slices = sorted(slices[self.start_slice:self.start_slice+self.num_slices], reverse=self.reverse_slices)\n",
    "\n",
    "        return slices\n",
    "\n",
    "    def make_mask(self, fragment_path):\n",
    "        mask_path = os.path.join(fragment_path, 'mask.png')\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        shape = (self.num_slices, mask.shape[0], mask.shape[1])\n",
    "        padding = get_padding(mask.shape, self.tile_size)\n",
    "        mask_pad = np.pad(mask, padding)\n",
    "\n",
    "        return mask_pad, shape, padding\n",
    "\n",
    "    def make_image(self, fragment_path, shape, padding):\n",
    "        image = np.zeros(shape=shape, dtype=np.uint8)\n",
    "        slices_files = sorted(glob(os.path.join(fragment_path, 'surface_volume/*.tif')))\n",
    "        slices_path = [slices_files[i] for i in self.slices]\n",
    "\n",
    "        print(f'\\nMake image from {fragment_path}')\n",
    "        for i, slice_path in tqdm(enumerate(slices_path), total=len(slices_path)):\n",
    "            image[i, ...] = cv2.imread(slice_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        padding.insert(0, (0, 0))\n",
    "        image_pad = np.pad(image, padding)\n",
    "\n",
    "        return image_pad\n",
    "\n",
    "    def create_items(self, fragment, mask_pad):\n",
    "        items = []\n",
    "        overlap_size = int(self.overlap * self.tile_size)\n",
    "        x_list = np.arange(0, mask_pad.shape[1] - overlap_size, overlap_size).tolist()\n",
    "        y_list = np.arange(0, mask_pad.shape[0] - overlap_size, overlap_size).tolist()\n",
    "\n",
    "        for x in x_list:\n",
    "            for y in y_list:\n",
    "                bbox = torch.IntTensor([x, y, x + self.tile_size, y + self.tile_size])\n",
    "                x0, y0, x1, y1 = bbox\n",
    "                tile = mask_pad[y0:y1, x0:x1]\n",
    "\n",
    "                if tile.sum() / (255 * self.tile_size ** 2) >= self.selection_thr:\n",
    "                    items.append({'fragment': fragment, 'bbox': bbox})\n",
    "\n",
    "        return items\n",
    "\n",
    "    def make_data(self):\n",
    "        data = {}\n",
    "        items = []\n",
    "\n",
    "        for fragment in self.fragments:\n",
    "            fragment_path = os.path.join(self.set_path, str(fragment))\n",
    "            mask_pad, shape, padding = self.make_mask(fragment_path)\n",
    "            image_pad = self.make_image(fragment_path, shape, padding)\n",
    "            items += self.create_items(fragment, mask_pad)\n",
    "\n",
    "            data[fragment] = {\n",
    "                'image': torch.from_numpy(image_pad).to(self.device)\n",
    "            }\n",
    "\n",
    "        return data, items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fragment, bbox = self.items[idx]['fragment'], self.items[idx]['bbox']\n",
    "        x0, y0, x1, y1 = bbox\n",
    "        image = self.data[fragment]['image'][:, y0:y1, x0:x1] / 255.0\n",
    "\n",
    "        return fragment, bbox, image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vesuvius Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul weight for each kfold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start_slice\n",
       "0     0.153861\n",
       "8     0.154643\n",
       "16    0.172690\n",
       "24    0.184504\n",
       "32    0.180550\n",
       "40    0.153751\n",
       "Name: val/sub_f05_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weight = df.groupby('start_slice')['val/sub_f05_score'].mean()\n",
    "df_weight /= df_weight.sum()\n",
    "display(df_weight)\n",
    "df_weight.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataset parameters for a given cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fragments': ['a', 'b'],\n",
       " 'tile_size': 256,\n",
       " 'num_slices': 16,\n",
       " 'start_slice': 16,\n",
       " 'slices_list': None,\n",
       " 'reverse_slices': False,\n",
       " 'selection_thr': 0,\n",
       " 'augmentation': False,\n",
       " 'device': device(type='mps'),\n",
       " 'overlap': 0.5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset_parameters(start_slice, df):\n",
    "    device = get_device()\n",
    "    dataset_params = {\n",
    "        \"fragments\": ['a', 'b'],\n",
    "        \"tile_size\": df['tile_size'][0],\n",
    "        \"num_slices\": df['num_slices'][0],\n",
    "        \"start_slice\": start_slice,\n",
    "        \"slices_list\": None,\n",
    "        \"reverse_slices\": df['reverse_slices'][0],\n",
    "        \"selection_thr\": 0,\n",
    "        \"augmentation\": False,\n",
    "        \"device\": device,\n",
    "        \"overlap\": 0.5,\n",
    "    }\n",
    "    return dataset_params\n",
    "\n",
    "get_dataset_parameters(16, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/vibrant-sweep-17-u21mnb2m-32-16-3.ckpt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_checkpoint(id):\n",
    "    return glob(join(MODELS_DIR, f'*{id}*.ckpt'))[0]\n",
    "\n",
    "find_checkpoint('u21mnb2m')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get lightning model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_params': {'in_channels': 16,\n",
       "  'encoder_weights': 'imagenet',\n",
       "  'classes': 1}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lightning_parameters(id, df):\n",
    "    lightning_params = {\n",
    "        # \"model_name\":  'efficientnet-b5',\n",
    "        \"model_params\":  {\n",
    "            \"in_channels\": df.loc[id, 'num_slices'],\n",
    "            \"encoder_weights\": df.loc[id, 'encoder_weights'],\n",
    "            \"classes\": 1\n",
    "        },\n",
    "        # \"learning_rate\":  df.loc[id, 'learning_rate'],\n",
    "        # \"bce_weight\":  df.loc[id, 'bce_weight'],\n",
    "        # \"dice_threshold\":  df.loc[id, 'dice_threshold'],    \n",
    "    }\n",
    "\n",
    "    return lightning_params\n",
    "\n",
    "get_lightning_parameters('u21mnb2m', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blank_masks(dataset: DatasetVesuvius):\n",
    "    blank_masks = {fragment_id: {} for fragment_id in dataset.fragments}\n",
    "    \n",
    "    for fragment_id in dataset.fragments:\n",
    "        inklabels_shape = get_fragment_shape(dataset.set_path, fragment_id, dataset.tile_size)\n",
    "        blank_masks[fragment_id]['inklabels'] = torch.zeros(inklabels_shape).cpu()\n",
    "        blank_masks[fragment_id]['count_map'] = torch.zeros(inklabels_shape).cpu()\n",
    "    \n",
    "    return blank_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_inklabels(masks, fragment_id, bbox, inklabels):\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    masks[fragment_id]['inklabels'][y0:y1, x0:x1] += inklabels\n",
    "    masks[fragment_id]['count_map'][y0:y1, x0:x1] += 1\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sub_prediction(dataset: DatasetVesuvius, model, weight):\n",
    "    masks = get_blank_masks(dataset)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    model.eval()\n",
    "    \n",
    "    for fragment_id, bbox, image in tqdm(dataset, leave=False, desc=\"Inference\"):\n",
    "        image = torch.unsqueeze(image, dim=0)\n",
    "        sub_inklabels = model(image)\n",
    "        sub_inklabels = sigmoid(sub_inklabels)\n",
    "        sub_inklabels = torch.squeeze(sub_inklabels)\n",
    "        masks = add_inklabels(masks, fragment_id, bbox, sub_inklabels.cpu())\n",
    "    \n",
    "    inklabels = {}\n",
    "    for fragment_id in dataset.fragments:\n",
    "        inklabels[fragment_id] = masks[fragment_id]['inklabels'] / masks[fragment_id]['count_map']\n",
    "        inklabels[fragment_id] = torch.nan_to_num(inklabels[fragment_id], nan=0)\n",
    "        inklabels[fragment_id] /= weight\n",
    "    \n",
    "    return inklabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2727, 6330])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_blank_inklabels(fragments, fragment_path):\n",
    "    blank_inklabels = {}\n",
    "    \n",
    "    for fragment_id in fragments:\n",
    "        shape = cv2.imread(join(fragment_path, fragment_id, 'mask.png')).shape\n",
    "        blank_inklabels[fragment_id] = torch.zeros(shape[:-1])\n",
    "    \n",
    "    return blank_inklabels\n",
    "\n",
    "masks = get_blank_inklabels(['a', 'b'], TEST_FRAGMENTS_PATH)\n",
    "display(masks['a'].shape)\n",
    "del masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Make image from ../data/raw/test/a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/_2tldmgn73jdx8x5snyxsnvw0000gn/T/ipykernel_40024/2100970209.py:42: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  slices = sorted(slices[self.start_slice:self.start_slice+self.num_slices], reverse=self.reverse_slices)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1f1a8774734ce5b079af6eebf068db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Make image from ../data/raw/test/b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d9d2c9639042c885c89159af0dce12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d30bdae62442b6bbf85750f0ad3f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/3468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 36.00 GB, other allocations: 296.72 MB, max allowed: 36.27 GB). Tried to allocate 4.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m model_pytorch \u001b[39m=\u001b[39m model_ligthning\u001b[39m.\u001b[39mmodel\n\u001b[1;32m     45\u001b[0m model_pytorch\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 47\u001b[0m sub_inklabels \u001b[39m=\u001b[39m make_sub_prediction(dataset, model_pytorch, weight)\n\u001b[1;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m fragment_id \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mfragments:\n\u001b[1;32m     50\u001b[0m     inklabels[fragment_id] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sub_inklabels[fragment_id]\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mmake_sub_prediction\u001b[0;34m(dataset, model, weight)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m fragment_id, bbox, image \u001b[39min\u001b[39;00m tqdm(dataset, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInference\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      7\u001b[0m     image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(image, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     sub_inklabels \u001b[39m=\u001b[39m model(image)\n\u001b[1;32m      9\u001b[0m     sub_inklabels \u001b[39m=\u001b[39m sigmoid(sub_inklabels)\n\u001b[1;32m     10\u001b[0m     sub_inklabels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(sub_inklabels)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vesuvius-challenge-ink-detection-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vesuvius-challenge-ink-detection-env/lib/python3.8/site-packages/segmentation_models_pytorch/base/model.py:29\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m---> 29\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     30\u001b[0m decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m     32\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vesuvius-challenge-ink-detection-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vesuvius-challenge-ink-detection-env/lib/python3.8/site-packages/segmentation_models_pytorch/encoders/resnet.py:62\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m features \u001b[39m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_depth \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     x \u001b[39m=\u001b[39m stages[i](x)\n\u001b[1;32m     63\u001b[0m     features\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vesuvius-challenge-ink-detection-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vesuvius-challenge-ink-detection-env/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vesuvius-challenge-ink-detection-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vesuvius-challenge-ink-detection-env/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vesuvius-challenge-ink-detection-env/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 36.00 GB, other allocations: 296.72 MB, max allowed: 36.27 GB). Tried to allocate 4.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model_temoins = None\n",
    "inklabels = get_blank_inklabels(['a', 'b'], TEST_FRAGMENTS_PATH)\n",
    "device = get_device()\n",
    "\n",
    "for start_slice, weight in df_weight.items():\n",
    "    dataset_params = get_dataset_parameters(start_slice, df)\n",
    "    dataset = DatasetVesuvius(**dataset_params)\n",
    "    \n",
    "    run_ids = df[df['start_slice'] == start_slice].index\n",
    "    for run_id in run_ids.to_numpy():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        lightning_params = get_lightning_parameters(run_id, df)\n",
    "        model_ligthning = LightningVesuvius(**lightning_params)\n",
    "        \n",
    "        model_path = find_checkpoint(run_id)\n",
    "        model_ligthning.load_from_checkpoint(\n",
    "            model_path, \n",
    "            map_location='cpu',\n",
    "            **lightning_params\n",
    "        )\n",
    "        model_pytorch = model_ligthning.model\n",
    "        model_pytorch.to(device=device)\n",
    "        \n",
    "        sub_inklabels = make_sub_prediction(dataset, model_pytorch, weight)\n",
    "        \n",
    "        for fragment_id in dataset.fragments:\n",
    "            inklabels[fragment_id] += sub_inklabels[fragment_id]\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesuvius-challenge-ink-detection-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
